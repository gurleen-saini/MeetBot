{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293961a3",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyMuPDF\n",
    "%pip install langchain\n",
    "%pip install -U sentence-transformers\n",
    "%pip install rank-bm25\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccf719",
   "metadata": {},
   "source": [
    "# PDF Text Extraction (Using PyMuPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45898e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Extract text\n",
    "pdf_text = extract_text_from_pdf(\"ds1.pdf\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796c2ee",
   "metadata": {},
   "source": [
    "# Chunk Text Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34389a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_text(pdf_text)\n",
    "\n",
    "# Save for later use\n",
    "with open(\"text_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump((chunks, [{\"text\": chunk} for chunk in chunks]), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dda2b3",
   "metadata": {},
   "source": [
    "# Initialize ChromaDB + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Init ChromaDB\n",
    "client = PersistentClient(path=\"./chromadb\")\n",
    "db = client.get_or_create_collection(\"text_embeddings\")\n",
    "\n",
    "# Load chunks\n",
    "with open(\"text_chunks.pkl\", \"rb\") as f:\n",
    "    chunk_texts, chunk_metadata = pickle.load(f)\n",
    "\n",
    "# Tokenize for BM25\n",
    "tokenized_docs = [text.split() for text in chunk_texts]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Store in ChromaDB\n",
    "for i, text in enumerate(chunk_texts):\n",
    "    embedding = embedding_model.encode(text, convert_to_numpy=True).tolist()\n",
    "    db.add(ids=[str(i)], embeddings=[embedding], metadatas=[{\"text\": text}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d58fff",
   "metadata": {},
   "source": [
    "# Hybrid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eccc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, chroma_db, bm25, k=5):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "\n",
    "    # Semantic (ChromaDB)\n",
    "    chromadb_results = chroma_db.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k\n",
    "    )[\"metadatas\"][0]\n",
    "    chromadb_texts = [result[\"text\"] for result in chromadb_results]\n",
    "\n",
    "    # BM25 (Lexical)\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "    bm25_results = [chunk_texts[i] for i in top_bm25_indices]\n",
    "\n",
    "    # Merge and deduplicate\n",
    "    combined_results = list(set(chromadb_texts + bm25_results))\n",
    "    return combined_results[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b9bcf",
   "metadata": {},
   "source": [
    "# Answer Generation Using LLaMA 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa6b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "login(hf_token)\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", token=hf_token)\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant that answers questions using the given context.\n",
    "ONLY use the information from the context. If the answer is not in the context, reply \"I don't know.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    result = qa_pipeline(prompt, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50)\n",
    "    return result[0][\"generated_text\"].split(\"### Answer:\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f1dd7",
   "metadata": {},
   "source": [
    "# Confidence Check (Entailment Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d08e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "entailment_pipeline = hf_pipeline(\"text-classification\", model=\"roberta-large-mnli\", truncation=True)\n",
    "\n",
    "def check_answer_confidence(question, context, answer):\n",
    "    input_text = f\"Question: {question}\\nContext: {context}\\nAnswer: {answer}\"\n",
    "    input_text = input_text[:512]  # Truncate\n",
    "    result = entailment_pipeline(input_text)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777e12b",
   "metadata": {},
   "source": [
    "# Final Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "client = PersistentClient(path=\"./chromadb\")\n",
    "chroma_db = client.get_or_create_collection(\"text_embeddings\")\n",
    "\n",
    "question = \"What recognitions and awards were announced by the Vice Chancellor?\"\n",
    "\n",
    "# Retrieve relevant context\n",
    "retrieved_contexts = hybrid_search(question, chroma_db, bm25)\n",
    "combined_context = \" \".join(retrieved_contexts)\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(question, combined_context)\n",
    "\n",
    "# Verify confidence\n",
    "confidence_label, confidence_score = check_answer_confidence(question, combined_context, answer)\n",
    "\n",
    "if confidence_label != \"ENTAILMENT\" or confidence_score < 0.75:\n",
    "    print(\"\\nðŸ¤” The answer might be unreliable. Consider checking the source manually.\\n\")\n",
    "\n",
    "print(\"Answer:\\n\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
